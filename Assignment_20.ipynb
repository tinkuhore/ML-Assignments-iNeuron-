{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d933e081-043b-4497-9598-87464618a444",
   "metadata": {},
   "source": [
    "## 1. What is the underlying concept of Support Vector Machines?\n",
    "Support Vector Machines (SVM) is a powerful supervised machine learning algorithm used for both classification and regression tasks. The underlying concept of SVM is to find the best hyperplane that separates the data into different classes while maximizing the margin between the classes. The hyperplane is defined by a linear combination of input features, and the goal is to find the optimal hyperplane that maximizes the margin, which is the distance between the hyperplane and the nearest data points from each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341680a5-444d-424e-a600-6e7aed989d4a",
   "metadata": {},
   "source": [
    "## 2. What is the concept of a support vector?\n",
    "In SVM, support vectors are the data points that lie closest to the decision boundary (hyperplane) between different classes. These support vectors play a crucial role in defining the decision boundary and determining the SVM model's performance. Only the support vectors contribute to the determination of the hyperplane, while the other data points are irrelevant for the final decision boundary. Support vectors are crucial because they define the margin and influence the model's generalization ability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a51c366-72bd-4c70-ab6a-530bddaf3a7f",
   "metadata": {},
   "source": [
    "## 3. When using SVMs, why is it necessary to scale the inputs?\n",
    "Scaling the inputs is necessary when using SVMs because SVMs are sensitive to the scale of the input features. If the features have different scales, the SVM algorithm may give more importance to features with larger scales, leading to biased results. Scaling the inputs ensures that all features are on a similar scale and have equal importance during the model training process. Common scaling techniques include standardization (mean = 0, standard deviation = 1) or normalization (scaling features to a specific range, e.g., [0, 1])."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456479c5-9020-40ee-b668-c9cb7e95455e",
   "metadata": {},
   "source": [
    "## 4. When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance?\n",
    "Yes, an SVM classifier can output a confidence score for its predictions. The confidence score represents the distance from the decision boundary or the margin. A higher confidence score indicates a higher confidence in the predicted class label. However, SVMs do not provide direct probability estimates like some other classifiers (e.g., logistic regression or Naive Bayes). To obtain probability estimates from an SVM classifier, we can use methods like Platt scaling or isotonic regression to convert the confidence scores into probability values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0639e5-5135-4544-8a5f-c28e8f9fa4e7",
   "metadata": {},
   "source": [
    "## 5. Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual form of the SVM problem?\n",
    "When dealing with a large dataset with millions of instances and hundreds of features, it is generally more efficient to use the dual form of the SVM problem. The dual form allows for more efficient computation by using the kernel trick, which implicitly maps the input features into a higher-dimensional space. The primal form directly optimizes the hyperplane parameters, whereas the dual form optimizes the coefficients associated with the support vectors. The dual form is particularly advantageous when the number of instances is larger than the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722f7820-eb58-428e-a968-3be3844ea79f",
   "metadata": {},
   "source": [
    "## 6. Let's say you've used an RBF kernel to train an SVM classifier, but it appears to underfit the training collection. Is it better to raise or lower (gamma)? What about the letter C?\n",
    "If the SVM classifier with an RBF kernel is underfitting the training data, you can try increasing the value of gamma. The gamma parameter controls the influence of a single training example. A higher value of gamma makes the decision boundary more flexible and can capture intricate patterns in the data. By increasing gamma, you allow the SVM to fit the training data more closely, potentially addressing underfitting. However, increasing gamma too much can lead to overfitting, where the model becomes too sensitive to the training data and fails to generalize well to unseen data. Therefore, it is important to choose an optimal value of gamma through experimentation and cross-validation.\n",
    "\n",
    "Regarding the C parameter, it controls the trade-off between achieving a larger margin and allowing more training examples to be misclassified. Increasing the value of C makes the SVM classifier focus more on correctly classifying the training examples rather than maximizing the margin. If the classifier is underfitting, reducing the value of C can be beneficial as it allows for a larger margin and more tolerance for misclassifications. By reducing C, you encourage the model to have a simpler decision boundary and avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7474da7b-c2b0-458c-bd73-b06fca0b0a45",
   "metadata": {},
   "source": [
    "## 7. To solve the soft-margin linear SVM classifier problem with an off-the-shelf QP solver, how should the QP parameters (H, f, A, and b) be set?\n",
    "To solve the soft-margin linear SVM classifier problem using a Quadratic Programming (QP) solver, the QP parameters need to be set as follows:\n",
    "\n",
    "I) H matrix: The H matrix should be an n x n positive definite matrix, where n is the number of features. For a linear SVM classifier, H is usually the identity matrix or a scaled version of it.\n",
    "\n",
    "II) f vector: The f vector is an n-dimensional vector that represents the linear term in the objective function. It is typically set to zeros or a small constant value.\n",
    "\n",
    "III) A matrix: The A matrix is an m x n matrix that represents the linear equality constraints. For a soft-margin SVM classifier, the A matrix is constructed from the training examples and their corresponding labels.\n",
    "\n",
    "IV) b vector: The b vector is an m-dimensional vector that represents the right-hand side of the equality constraints. It is set to -1 for all training examples belonging to the negative class and +1 for all training examples belonging to the positive class.\n",
    "\n",
    "The specific construction of the A matrix and b vector depends on the formulation of the soft-margin SVM problem and the chosen optimization library or solver. The objective is to find the values of the primal variables (weights and biases) that minimize the objective function while satisfying the constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd723d70-5228-428a-ae4b-3a971f1e2bf7",
   "metadata": {},
   "source": [
    "## 8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and an SGDClassifier. See if you can get them to make a model that is similar to yours.\n",
    "To train linear classifiers like LinearSVC, SVC (with a linear kernel), and SGDClassifier on a linearly separable dataset, you can follow these steps:\n",
    "\n",
    "Load the dataset: Load your linearly separable dataset into your programming environment. Ensure that you have separate arrays for the input features (X) and the corresponding target labels (y).\n",
    "\n",
    "Split the dataset: Split the dataset into a training set and a test set using a train-test split function or cross-validation techniques. This step ensures that you can evaluate the performance of the trained models on unseen data.\n",
    "\n",
    "Train the LinearSVC model: Create an instance of the LinearSVC class, specifying any desired hyperparameters. Fit the model to the training data using the fit method, passing the input features (X_train) and target labels (y_train).\n",
    "\n",
    "Train the SVC model: Create an instance of the SVC class with a linear kernel, specifying any desired hyperparameters. Fit the model to the training data using the fit method, passing X_train and y_train.\n",
    "\n",
    "Train the SGDClassifier: Create an instance of the SGDClassifier class with a loss function suitable for linear classification (e.g., \"hinge\"). Specify any desired hyperparameters. Fit the model to the training data using the fit method, passing X_train and y_train.\n",
    "\n",
    "Evaluate the models: Use the trained models to make predictions on the test set (X_test) using the predict method. Compare the predictions with the true labels (y_test) to evaluate the models' performance. You can use metrics such as accuracy, precision, recall, or F1-score to assess their similarity.\n",
    "\n",
    "It's worth noting that the LinearSVC and SVC models aim to find the maximum-margin hyperplane, while the SGDClassifier uses stochastic gradient descent to optimize the objective function. Due to implementation differences, the decision boundaries and model parameters may not be identical, but they should be similar in terms of separating the linearly separable data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8f751b-e165-4256-bc02-424cf7be9084",
   "metadata": {},
   "source": [
    "## 9. On the MNIST dataset, train an SVM classifier. You'll need to use one-versus-the-rest to assign all 10 digits because SVM classifiers are binary classifiers. To accelerate the process, you might want to tune the hyperparameters using small validation sets. What level of precision can you achieve?\n",
    "\n",
    "Training an SVM classifier on the MNIST dataset requires implementing a one-versus-the-rest (OvR) approach since SVM classifiers are binary classifiers. Here's a step-by-step guide to training an SVM classifier on the MNIST dataset and evaluating its precision:\n",
    "\n",
    "1. Load the MNIST dataset: Load the MNIST dataset into your programming environment. Ensure that you have separate arrays for the input features (X) and the corresponding target labels (y).\n",
    "\n",
    "2. Split the dataset: Split the dataset into training, validation, and test sets. The training set will be used for model training, the validation set for hyperparameter tuning, and the test set for evaluating the final model performance. You can use the train-test split function or cross-validation techniques to achieve this.\n",
    "\n",
    "3. Preprocess the data: Preprocess the input features (X) if necessary. Common preprocessing steps include scaling the features to a similar range, normalizing the values, or applying dimensionality reduction techniques.\n",
    "\n",
    "4. Implement OvR strategy: Since SVM classifiers are binary classifiers, you need to train multiple classifiers using the OvR strategy. For each digit (0-9), train a separate SVM classifier where the positive class is the current digit, and all other digits are considered negative. This results in 10 binary classifiers.\n",
    "\n",
    "5. Hyperparameter tuning: To optimize the performance of each binary classifier, tune the hyperparameters using the validation set. Some commonly tuned hyperparameters for SVMs include the choice of kernel, regularization parameter (C), and kernel-specific parameters (e.g., gamma for RBF kernel). Use techniques like grid search or random search to explore different hyperparameter combinations and select the best-performing ones.\n",
    "\n",
    "6. Train the SVM classifiers: Train each SVM classifier using the selected hyperparameters and the training set. Fit the model to the corresponding subset of data for each binary classification task.\n",
    "\n",
    "7. Evaluate the model: After training the SVM classifiers, evaluate their performance on the test set. Calculate the precision metric for each digit by comparing the predicted labels with the true labels. Precision measures the proportion of correctly predicted positive instances (digits) out of all instances predicted as positive.\n",
    "\n",
    "Here's a code snippet demonstrating the training and evaluation process using scikit-learn's SVM implementation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "140a3461-a564-4452-9365-18d8c31aa929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tinku/.local/lib/python3.10/site-packages/sklearn/datasets/_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST dataset shape:\n",
      "Input features (X): (70000, 784)\n",
      "Target labels (y): (70000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "\n",
    "# Access the input features (X) and target labels (y)\n",
    "X = mnist.data\n",
    "y = mnist.target\n",
    "\n",
    "# Print the shape of the dataset\n",
    "print(\"MNIST dataset shape:\")\n",
    "print(\"Input features (X):\", X.shape)\n",
    "print(\"Target labels (y):\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c337b9a-71bd-4240-af7b-1ce63213f428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data if necessary (e.g., scaling)\n",
    "\n",
    "# Implement OvR strategy and train SVM classifiers\n",
    "svm_classifiers = []\n",
    "for digit in range(10):\n",
    "    svm = SVC(kernel='rbf', C=1.0, gamma='scale')  # Adjust hyperparameters as needed\n",
    "    # Prepare binary labels for current digit\n",
    "    y_train_binary = (y_train == str(digit))\n",
    "    # Train the SVM classifier\n",
    "    svm.fit(X_train, y_train_binary)\n",
    "    svm_classifiers.append(svm)\n",
    "\n",
    "# Evaluate precision on the test set\n",
    "y_pred = []\n",
    "for svm in svm_classifiers:\n",
    "    y_pred_digit = svm.predict(X_test)\n",
    "    y_pred.append(y_pred_digit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18c02839-0123-43c4-9435-23b93b36fb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for digit 0 ->> 0.9056428571428572\n",
      "Accuracy for digit 1 ->> 0.8863571428571428\n",
      "Accuracy for digit 2 ->> 0.9042857142857142\n",
      "Accuracy for digit 3 ->> 0.9022142857142857\n",
      "Accuracy for digit 4 ->> 0.9090714285714285\n",
      "Accuracy for digit 5 ->> 0.9128571428571428\n",
      "Accuracy for digit 6 ->> 0.9017142857142857\n",
      "Accuracy for digit 7 ->> 0.8957857142857143\n",
      "Accuracy for digit 8 ->> 0.9082857142857143\n",
      "Accuracy for digit 9 ->> 0.9027857142857143\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy for each digit\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for digit in range(10):\n",
    "    accuracy = accuracy_score(y_test==digit, y_pred[digit])\n",
    "    print(f\"Accuracy for digit {digit} ->> {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ade8f9d-6bb6-4525-8173-9116e1cc08b9",
   "metadata": {},
   "source": [
    "## 10. On the California housing dataset, train an SVM regressor.\n",
    "\n",
    "To train an SVM regressor on the California housing dataset, you can follow these steps:\n",
    "\n",
    "Load the California housing dataset: Load the California housing dataset into your programming environment. Ensure that you have separate arrays for the input features (X) and the corresponding target variable (y).\n",
    "\n",
    "Split the dataset: Split the dataset into a training set and a test set using a train-test split function or cross-validation techniques. This step ensures that you can evaluate the performance of the trained regressor on unseen data.\n",
    "\n",
    "Preprocess the data: Depending on the specific SVM implementation and the characteristics of the dataset, you may need to preprocess the data. Common preprocessing steps include scaling the input features to a similar range and handling missing values if present.\n",
    "\n",
    "Train the SVM regressor: Create an instance of the SVM regressor, specifying the desired kernel and other hyperparameters. Fit the model to the training data using the fit method, passing X_train and y_train.\n",
    "\n",
    "Here's an example code snippet using scikit-learn's SVM regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54119992-e64b-4147-8b65-4332b4e0b612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (20640, 8)\n",
      "Shape of y: (20640,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load the California housing dataset\n",
    "data = fetch_california_housing()\n",
    "\n",
    "# Extract the feature matrix and target vector\n",
    "X = data.data  # Feature matrix\n",
    "y = data.target  # Target vector\n",
    "\n",
    "# Print the shape of the feature matrix and target vector\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "936764e8-9900-4c31-a73f-c72a5ed2bdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.3570026426754463\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data by scaling the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the SVM regressor\n",
    "svr = SVR(kernel='rbf', C=1.0, gamma='scale')\n",
    "svr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svr.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model using mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63d5e35-029b-4f54-b437-a6153a6e6d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c6b24-a3fb-4b54-bde6-7fbb0f41dc68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
