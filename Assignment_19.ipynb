{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11859b1c-fc04-4e7f-b56e-8ebb5c382c0b",
   "metadata": {},
   "source": [
    "# 1. A set of one-dimensional data points is given to you: 5, 10, 15, 20, 25, 30, 35. Assume that k = 2 and that the first set of random centroid is 15, 32, and that the second set is 12, 30.\n",
    "# a) Using the k-means method, create two clusters for each set of centroid described above.\n",
    "# b) For each set of centroid values, calculate the SSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f2206-88b8-4782-b03d-328b982181b8",
   "metadata": {},
   "source": [
    "### a) Using the k-means method to create two clusters for each set of centroids:\n",
    "\n",
    "**Set 1: Centroids = {15, 32}**\n",
    "\n",
    "Assigning data points to clusters:\n",
    "\n",
    "Data points: 5, 10, 15, 20, 25, 30, 35\n",
    "\n",
    "\n",
    "Calculating distances from centroids:\n",
    "\n",
    "Distance from 15: {0, 5, 0, 5, 10, 15, 20}\n",
    "\n",
    "Distance from 32: {17, 22, 17, 12, 7, 2, 3}\n",
    "\n",
    "\n",
    "Assigning data points to clusters based on minimum distance:\n",
    "\n",
    "Cluster 1: {5, 10, 15, 20}\n",
    "\n",
    "Cluster 2: {25, 30, 35}\n",
    "\n",
    "\n",
    "**Set 2: Centroids = {12, 30}**\n",
    "\n",
    "Assigning data points to clusters:\n",
    "\n",
    "Data points: 5, 10, 15, 20, 25, 30, 35\n",
    "\n",
    "\n",
    "Calculating distances from centroids:\n",
    "\n",
    "Distance from 12: {7, 2, 3, 8, 13, 18, 23}\n",
    "\n",
    "Distance from 30: {25, 20, 15, 10, 5, 0, 5}\n",
    "\n",
    "\n",
    "Assigning data points to clusters based on minimum distance:\n",
    "\n",
    "Cluster 1: {5, 10, 15, 20}\n",
    "\n",
    "Cluster 2: {25, 30, 35}\n",
    "\n",
    "\n",
    "### b) Calculating SSE for each set of centroid values:\n",
    "\n",
    "**Set 1 SSE calculation:**\n",
    "\n",
    "Cluster 1 (Centroid: 15):\n",
    "\n",
    "Data points: 5, 10, 15, 20\n",
    "\n",
    "Next Centroid = Mean of all Data points: (5 + 10 + 15 + 20) / 4 = 12.5\n",
    "\n",
    "Squared errors: (5 - 15)^2 + (10 - 15)^2 + (15 - 15)^2 + (20 - 15)^2 = 150\n",
    "\n",
    "\n",
    "Cluster 2 (Centroid: 32):\n",
    "\n",
    "Data points: 25, 30, 35\n",
    "\n",
    "Next Centroid = Mean of all Data points: (25 + 30 + 35) / 3 = 30\n",
    "\n",
    "Squared errors: (25 - 32)^2 + (30 - 32)^2 + (35 - 32)^2 = 62\n",
    "\n",
    "\n",
    "Total SSE: 150 + 62 = 212\n",
    "\n",
    "\n",
    "**Set 2 SSE calculation:**\n",
    "\n",
    "Cluster 1 (Centroid: 12):\n",
    "\n",
    "Data points: 5, 10, 15, 20\n",
    "\n",
    "Mean: (5 + 10 + 15 + 20) / 4 = 12.5\n",
    "\n",
    "Squared errors: (5 - 12)^2 + (10 - 12)^2 + (15 - 12)^2 = 62\n",
    "\n",
    "\n",
    "Cluster 2 (Centroid: 30):\n",
    "\n",
    "Data points: 25, 30, 35\n",
    "\n",
    "Mean: (25 + 30 + 35) / 3 = 30\n",
    "\n",
    "Squared errors: (20 - 30)^2 + (25 - 30)^2 + (30 - 30)^2 + (35 - 30)^2 = 150\n",
    "\n",
    "\n",
    "Total SSE: 62 + 150 = 212\n",
    "\n",
    "Therefor SSE for both sets is 212."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee1a22b-91ec-4c2d-81d3-f7db8ca87019",
   "metadata": {},
   "source": [
    "# 2. Describe how the Market Basket Research makes use of association analysis concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988dd200-e7b4-4e34-8d0e-435ba5fb7178",
   "metadata": {},
   "source": [
    "Market Basket Analysis (MBA) makes use of association analysis concepts to uncover relationships and patterns among items that are frequently purchased together. It helps businesses understand the buying behavior of customers and identify associations between products or services. Here's how association analysis is applied in Market Basket Research:\n",
    "\n",
    "Apriori Algorithm: The Apriori algorithm is commonly used in association analysis for Market Basket Research. It works by identifying frequent itemsets, which are combinations of items that occur together in a given dataset above a specified support threshold. The algorithm generates rules based on the itemsets, allowing businesses to understand which items tend to be purchased together.\n",
    "\n",
    "Support: Support is a key concept in association analysis. It measures the frequency or occurrence of an itemset in a dataset. In Market Basket Research, support indicates how frequently a particular combination of items appears in customer transactions. Items with high support are considered popular or frequently purchased together.\n",
    "\n",
    "Confidence: Confidence is another important concept in association analysis. It measures the strength of an association rule between two or more items. In Market Basket Research, confidence represents the likelihood that customers who buy one item will also buy another item. Higher confidence values indicate stronger associations between items.\n",
    "\n",
    "Association Rules: Association rules are generated based on the frequent itemsets and their corresponding support and confidence values. These rules provide insights into the relationships between items in the dataset. For example, a common association rule could be \"If a customer buys item A, they are likely to buy item B with a confidence of X%.\"\n",
    "\n",
    "Lift: Lift is a metric used to measure the strength and significance of an association rule. It indicates how much more likely the items in a rule are purchased together compared to their individual probabilities. High lift values suggest a strong association between items, while lift values close to 1 indicate independence.\n",
    "\n",
    "By applying association analysis techniques like the Apriori algorithm and examining support, confidence, and lift values, Market Basket Research enables businesses to understand customer behavior, optimize product placement, design targeted marketing campaigns, and improve sales strategies. It helps uncover meaningful associations between items, leading to valuable insights and actionable recommendations for businesses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d021ed82-b68c-46e5-bb5f-1c560762fd69",
   "metadata": {},
   "source": [
    "# 3. Give an example of the Apriori algorithm for learning association rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ff65f9-c79d-4020-aad2-7a5a79a985b6",
   "metadata": {},
   "source": [
    "Let's consider a dataset of customer transactions at a grocery store. The dataset contains information about the items purchased by customers. Here's an example of how the Apriori algorithm can be used to learn association rules:\n",
    "\n",
    "Step 1: Data Preparation\n",
    "Assume we have the following transactions:\n",
    "Transaction 1: {Bread, Milk}\n",
    "Transaction 2: {Bread, Butter}\n",
    "Transaction 3: {Bread, Cheese}\n",
    "Transaction 4: {Milk, Butter}\n",
    "Transaction 5: {Milk, Cheese}\n",
    "\n",
    "Step 2: Set Minimum Support and Confidence Thresholds\n",
    "Define minimum support and confidence thresholds, for example:\n",
    "Minimum support: 20% (occurrence in at least 20% of transactions)\n",
    "Minimum confidence: 50% (for generating association rules)\n",
    "\n",
    "Step 3: Generate Frequent Itemsets\n",
    "Start with individual items and calculate their support:\n",
    "Item: Support\n",
    "Bread: 3/5 = 60%\n",
    "Milk: 3/5 = 60%\n",
    "Butter: 2/5 = 40%\n",
    "Cheese: 2/5 = 40%\n",
    "\n",
    "Since all items meet the minimum support threshold, they are considered frequent itemsets.\n",
    "\n",
    "Step 4: Generate Candidate Itemsets\n",
    "Combine the frequent itemsets to generate larger candidate itemsets:\n",
    "Candidate Itemsets: {Bread, Milk}, {Bread, Butter}, {Bread, Cheese}, {Milk, Butter}, {Milk, Cheese}\n",
    "\n",
    "Step 5: Calculate Support for Candidate Itemsets\n",
    "Count the occurrences of each candidate itemset in the transactions:\n",
    "{Bread, Milk}: 2/5 = 40%\n",
    "{Bread, Butter}: 1/5 = 20%\n",
    "{Bread, Cheese}: 1/5 = 20%\n",
    "{Milk, Butter}: 1/5 = 20%\n",
    "{Milk, Cheese}: 1/5 = 20%\n",
    "\n",
    "Step 6: Prune Candidate Itemsets\n",
    "Remove candidate itemsets that do not meet the minimum support threshold:\n",
    "Pruned Itemsets: {Bread, Milk}\n",
    "\n",
    "Step 7: Generate Association Rules\n",
    "Generate association rules from the frequent itemsets:\n",
    "{Bread} => {Milk} (Support: 40%, Confidence: 67%)\n",
    "\n",
    "The generated association rule suggests that if a customer buys Bread, there is a 67% chance they will also buy Milk based on the dataset.\n",
    "\n",
    "The above example demonstrates a simple application of the Apriori algorithm. The process involves iterating through the steps of generating frequent itemsets, generating candidate itemsets, calculating support, pruning, and generating association rules. The algorithm helps uncover meaningful associations between items based on their support and confidence values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f39da7-5e22-4416-a628-a0a4c8a94e67",
   "metadata": {},
   "source": [
    "# 4. In hierarchical clustering, how is the distance between clusters measured? Explain how this metric is used to decide when to end the iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1bfc08-990e-4bf3-a55a-370d1b684b7d",
   "metadata": {},
   "source": [
    "In hierarchical clustering, the distance between clusters is measured using different distance metrics, such as Euclidean distance, Manhattan distance, or cosine similarity. The choice of distance metric depends on the nature of the data and the problem at hand.\n",
    "\n",
    "The most common distance metric used in hierarchical clustering is the Euclidean distance. It calculates the straight-line distance between two data points in the feature space. Other distance metrics, like Manhattan distance, consider the sum of the absolute differences between the coordinates of two points.\n",
    "\n",
    "To decide when to end the iteration and form the clusters, hierarchical clustering relies on the concept of a linkage criterion. The linkage criterion determines how the distance between clusters is calculated based on the distances between their constituent data points.\n",
    "\n",
    "There are different types of linkage criteria used in hierarchical clustering, including:\n",
    "\n",
    "Single Linkage: The distance between two clusters is defined as the minimum distance between any two points, one from each cluster. It focuses on the closest pair of points between clusters.\n",
    "Complete Linkage: The distance between two clusters is defined as the maximum distance between any two points, one from each cluster. It focuses on the farthest pair of points between clusters.\n",
    "Average Linkage: The distance between two clusters is defined as the average distance between all possible pairs of points, one from each cluster. It considers the average proximity between clusters.\n",
    "Ward's Linkage: It minimizes the increase in the total within-cluster variance when merging two clusters. It focuses on minimizing the variance within clusters.\n",
    "The choice of linkage criterion can affect the shape and structure of the resulting clusters. By comparing the distances between clusters and applying the linkage criterion, hierarchical clustering iteratively merges the closest or most similar clusters until a stopping criterion is met. The stopping criterion can be a predetermined number of clusters or a specific distance threshold.\n",
    "\n",
    "Once the iteration ends, the resulting dendrogram can be cut at a certain level to obtain the desired number of clusters. The specific level at which the dendrogram is cut determines the granularity of the clustering solution.\n",
    "\n",
    "Overall, the distance metric and linkage criterion play crucial roles in measuring the distance between clusters and deciding when to stop the iteration in hierarchical clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a284f4e-423c-4cdb-931e-2cb67b76116e",
   "metadata": {},
   "source": [
    "# 5. In the k-means algorithm, how do you recompute the cluster centroids?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae280648-c44c-43f3-bce0-df710894057b",
   "metadata": {},
   "source": [
    "In the k-means algorithm, the recomputation of cluster centroids is performed in the following steps:\n",
    "\n",
    "Initialization: Initially, the algorithm randomly assigns K centroid points in the feature space. These centroids represent the centers of the initial clusters.\n",
    "\n",
    "Assignment: Each data point in the dataset is assigned to the nearest centroid based on a distance metric, typically the Euclidean distance. This step forms K clusters, with each data point belonging to the cluster associated with its nearest centroid.\n",
    "\n",
    "Recomputation: After the data points have been assigned to clusters, the centroids of the clusters are recomputed to reflect the new cluster assignments. The centroid of a cluster is calculated as the mean or average of all data points assigned to that cluster.\n",
    "\n",
    "Mathematically, for each cluster, the centroid coordinates (c1, c2, ..., cn) are recalculated as the average of the coordinates (x1, x2, ..., xn) of all data points assigned to that cluster:\n",
    "\n",
    "ci = (x1 + x2 + ... + xn) / n\n",
    "\n",
    "where ci represents the coordinate of the centroid dimension i, xi represents the coordinate of the data point dimension i, and n is the number of data points in the cluster.\n",
    "\n",
    "Iteration: Steps 2 and 3 are repeated iteratively until convergence. Convergence occurs when the cluster assignments and centroids no longer change significantly or when a predetermined number of iterations is reached.\n",
    "\n",
    "By recomputing the cluster centroids in each iteration, the k-means algorithm gradually adjusts the centroids to better represent the centers of the clusters based on the current data point assignments. This process continues until the algorithm converges and the centroids stabilize, resulting in the final clustering solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d546ca1-33be-4bfe-a2b8-0b9c3424a1d9",
   "metadata": {},
   "source": [
    "# 6. At the start of the clustering exercise, discuss one method for determining the required number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379d6bb5-4e88-46bf-a78d-96d3b6ff8009",
   "metadata": {},
   "source": [
    "Determining the required number of clusters at the start of a clustering exercise is an important step to ensure meaningful and accurate results. There are several methods available to help determine the optimal number of clusters, and one commonly used approach is the \"elbow method.\" The elbow method involves plotting the number of clusters against the corresponding clustering performance metric and identifying the \"elbow\" point on the plot.\n",
    "\n",
    "Here's how the elbow method works:\n",
    "\n",
    "Perform the clustering algorithm for a range of cluster numbers (K) on the dataset.\n",
    "\n",
    "Calculate a clustering performance metric for each value of K. This metric could be the within-cluster sum of squares (WCSS), which measures the compactness of the clusters. The WCSS is calculated as the sum of squared distances between each data point and its centroid within the cluster.\n",
    "\n",
    "Plot the number of clusters (K) on the x-axis and the corresponding clustering performance metric (e.g., WCSS) on the y-axis.\n",
    "\n",
    "Examine the plot and look for the \"elbow\" point, which is the point of inflection or significant decrease in the rate of change of the clustering performance metric.\n",
    "\n",
    "If the plot shows a clear elbow point, it suggests that adding more clusters beyond that point may not provide significant improvement in clustering performance.\n",
    "\n",
    "If the plot does not exhibit a distinct elbow point, it can be challenging to determine the optimal number of clusters using this method alone.\n",
    "\n",
    "The elbow method provides a visual indication of the optimal number of clusters based on the trade-off between clustering performance and the complexity of the model. However, it's important to note that the choice of the number of clusters is subjective and depends on domain knowledge, specific problem requirements, and interpretation of the results. Therefore, it's recommended to use the elbow method as a starting point and consider other validation techniques and expert judgment to finalize the appropriate number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f6dad8-00d1-4157-b287-209ead4ef4af",
   "metadata": {},
   "source": [
    "# 7. Discuss the k-means algorithm's advantages and disadvantages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc9665b-d411-433e-849b-a267d6f8b3d7",
   "metadata": {},
   "source": [
    "The k-means algorithm has several advantages and disadvantages. Let's discuss them:\n",
    "\n",
    "**Advantages of the k-means algorithm:**\n",
    "\n",
    "- Simplicity: The k-means algorithm is relatively easy to understand and implement. It is a straightforward clustering algorithm that assigns data points to clusters based on their proximity to the cluster centroids.\n",
    "\n",
    "- Scalability: K-means can efficiently handle large datasets with a relatively low computational cost. It is suitable for applications with a large number of data points and dimensions.\n",
    "\n",
    "- Speed: K-means is known for its computational efficiency, making it an attractive choice for clustering large datasets. It converges quickly, especially when combined with initialization techniques like k-means++.\n",
    "\n",
    "- Interpretability: The results of k-means are easily interpretable. The algorithm assigns each data point to a specific cluster, allowing for straightforward analysis and understanding of the data distribution.\n",
    "\n",
    "**Disadvantages of the k-means algorithm:**\n",
    "\n",
    "- Sensitivity to initial centroid selection: The final clustering result can vary based on the initial centroid positions. Different initializations can lead to different cluster assignments and potentially suboptimal solutions. Using techniques like k-means++ for initialization can mitigate this issue to some extent.\n",
    "\n",
    "- Assumption of spherical clusters and equal-sized clusters: K-means assumes that clusters are spherical and have equal sizes. This assumption may not hold for complex datasets with irregularly shaped or overlapping clusters. K-means can struggle to capture non-linear relationships and can produce suboptimal results in such cases.\n",
    "\n",
    "- Determination of the number of clusters: The number of clusters (k) needs to be specified in advance. However, determining the optimal number of clusters is not always straightforward and can be subjective. Incorrectly choosing the number of clusters can lead to poor clustering results.\n",
    "\n",
    "- Sensitivity to outliers: K-means is sensitive to outliers as they can significantly influence the centroid positions. Outliers can pull centroids away from the true cluster centers, leading to suboptimal clustering results.\n",
    "\n",
    "- Restricted to numeric data: K-means is designed for numerical data and may not be suitable for categorical or mixed data types without appropriate preprocessing.\n",
    "\n",
    "It's important to consider these advantages and disadvantages when choosing the k-means algorithm for clustering tasks and assess whether it aligns with the specific requirements and characteristics of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b664759a-2955-4ec7-8cc1-3e98f0d7e77d",
   "metadata": {},
   "source": [
    "# 8. Draw a diagram to demonstrate the principle of clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acba1896-9aea-4e5c-9fc3-78df6fbb7919",
   "metadata": {},
   "source": [
    "![Principle of Clustering](clustering.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c50d21-db1a-4149-b4f3-848442094373",
   "metadata": {},
   "source": [
    "# 9. During your study, you discovered seven findings, which are listed in the data points below. Using the K-means algorithm, you want to build three clusters from these observations. The clusters C1, C2, and C3 have the following findings after the first iteration:\n",
    "\n",
    "# C1: (2,2), (4,4), (6,6); C2: (2,2), (4,4), (6,6); C3: (2,2), (4,4),\n",
    "\n",
    "# C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,\n",
    "\n",
    "# C3: (5,5) and (9,9)\n",
    "\n",
    "# What would the cluster centroids be if you were to run a second iteration? What would this clustering's SSE be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efa24e3-2435-4ad4-b5bb-4c4f999264a8",
   "metadata": {},
   "source": [
    "To determine the cluster centroids and SSE in the second iteration of the K-means algorithm, we need to reassign the data points to the nearest centroids and calculate the new centroids.\n",
    "\n",
    "In the given example, we have three clusters: C1, C2, and C3. The initial cluster centroids after the first iteration are as follows:\n",
    "\n",
    "C1: (2,2), (4,4), (6,6)\n",
    "C2: (2,2), (4,4), (6,6)\n",
    "C3: (2,2), (4,4)\n",
    "\n",
    "To calculate the new centroids, we find the mean of the data points within each cluster:\n",
    "\n",
    "New C1 centroid: Mean of (2,2), (4,4), (6,6) = (4,4)\n",
    "New C2 centroid: Mean of (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0 = (1.77, 2.88)\n",
    "New C3 centroid: Mean of (5,5), (9,9) = (7,7)\n",
    "\n",
    "The updated cluster centroids after the second iteration are:\n",
    "C1: (4,4)\n",
    "C2: (1.77, 2.88)\n",
    "C3: (7,7)\n",
    "\n",
    "To calculate the SSE (Sum of Squared Errors), we need to compute the squared Euclidean distance between each data point and its assigned centroid and sum them up for all data points:\n",
    "\n",
    "SSE = (Distance from (2,2) to C1)^2 + (Distance from (4,4) to C1)^2 + (Distance from (6,6) to C1)^2\n",
    "+ (Distance from (0,4) to C2)^2 + (Distance from (4,0) to C2)^2 + (Distance from (0,4) to C2)^2\n",
    "+ (Distance from (0,4) to C2)^2 + (Distance from (0,4) to C2)^2 + (Distance from (0,4) to C2)^2\n",
    "+ (Distance from (0,4) to C2)^2 + (Distance from (0,4) to C2)^2 + (Distance from (0,4) to C2)^2\n",
    "+ (Distance from (0,4) to C2)^2 + (Distance from (5,5) to C3)^2 + (Distance from (9,9) to C3)^2\n",
    "\n",
    "To calculate the distances, we use the Euclidean distance formula:\n",
    "Distance = sqrt((x2 - x1)^2 + (y2 - y1)^2)\n",
    "\n",
    "Substituting the values and calculating the distances, we obtain the SSE for this clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586584b3-42b3-440e-958d-bdd6a04725fe",
   "metadata": {},
   "source": [
    "# 10. In a software project, the team is attempting to determine if software flaws discovered during testing are identical. Based on the text analytics of the defect details, they decided to build 5 clusters of related defects. Any new defect formed after the 5 clusters of defects have been identified must be listed as one of the forms identified by clustering. A simple diagram can be used to explain this process. Assume you have 20 defect data points that are clustered into 5 clusters and you used the k-means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ab05e4-f8ba-459f-9069-c67a2491ba1f",
   "metadata": {},
   "source": [
    "    Cluster 1        Cluster 2        Cluster 3        Cluster 4        Cluster 5\n",
    "    +---------+      +---------+      +---------+      +---------+      +---------+\n",
    "    | Defect  |      | Defect  |      | Defect  |      | Defect  |      | Defect  |\n",
    "    |   1     |      |   6     |      |   11    |      |   16    |      |   19    |\n",
    "    +---------+      +---------+      +---------+      +---------+      +---------+\n",
    "    | Defect  |      | Defect  |      | Defect  |      | Defect  |      | Defect  |\n",
    "    |   2     |      |   7     |      |   12    |      |   17    |      |   20    |\n",
    "    +---------+      +---------+      +---------+      +---------+      +---------+\n",
    "    | Defect  |      | Defect  |      | Defect  |      | Defect  |      | Defect  |\n",
    "    |   3     |      |   8     |      |   13    |      |   18    |      |   ...   |\n",
    "    +---------+      +---------+      +---------+      +---------+      +---------+\n",
    "    | Defect  |      | Defect  |      | Defect  |      | Defect  |\n",
    "    |   4     |      |   9     |      |   14    |      |   ...   |\n",
    "    +---------+      +---------+      +---------+      +---------+\n",
    "    | Defect  |      | Defect  |      | Defect  |\n",
    "    |   5     |      |   10    |      |   15    |\n",
    "    +---------+      +---------+      +---------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c30485-8412-4b7e-b035-86d6ee3827b0",
   "metadata": {},
   "source": [
    "In this example, we have 20 defect data points that are clustered into 5 clusters (Cluster 1 to Cluster 5) using the k-means algorithm. Each defect is represented by a number. The defects within each cluster share similar characteristics or patterns based on the text analytics of the defect details.\n",
    "\n",
    "Once the 5 clusters of defects have been identified, any new defect that arises must be assigned to one of the existing clusters. This ensures that new defects are classified into one of the predefined forms identified by clustering.\n",
    "\n",
    "The diagram provides a visual representation of the clustered defects, with each cluster containing a group of related defects. This approach helps the software project team in categorizing and addressing similar types of defects efficiently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
